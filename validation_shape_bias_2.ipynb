{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import util.util_validation as ut_val\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "from networks.resnet_big import model_dict\n",
    "from util.util_logging import open_csv_file\n",
    "\n",
    "seaborn.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\"CE_baseline\": [\"./save/SupCE/animals10/SupCE_animals10_resnet18_lr_0.125_decay_0.0001_bsz_26_trial_0_baseline_cosine/models/last.pth\", None],\n",
    "               \"CE_origAllAug\": [\"./save/SupCE/animals10_diff_-1/SupCE_animals10_diff_-1_resnet18_lr_0.125_decay_0.0001_bsz_26_trial_0_origAllAug_cosine/models/last.pth\", None],\n",
    "               \"CE_diff4000\": [\"./save/SupCE/animals10_diff_4000/SupCE_animals10_diff_4000_resnet18_lr_0.125_decay_0.0001_bsz_26_trial_0_4000_cosine/models/last.pth\", None],\n",
    "               \"CE_diff4000AllAug\": [\"./save/SupCE/animals10_diff_4000/SupCE_animals10_diff_4000_resnet18_lr_0.125_decay_0.0001_bsz_26_trial_0_4000AllAug_cosine/models/ckpt_epoch_250.pth\", None],\n",
    "               \"CE_diff8000\": [\"./save/SupCE/animals10_diff_8000/SupCE_animals10_diff_8000_resnet18_lr_0.125_decay_0.0001_bsz_26_trial_0_8000_cosine/models/last.pth\", None],\n",
    "               \"CE_diffAug\": [\"./save/SupCE/animals10_diff_-1+4000/SupCE_animals10_diff_-1+4000_resnet18_lr_0.125_decay_0.0001_bsz_26_trial_0_diffAug_cosine/models/last.pth\", None],\n",
    "               \"CE_diffAugAllAug\": [\"./save/SupCE/animals10_diff_-1+4000/SupCE_animals10_diff_-1+4000_resnet18_lr_0.125_decay_0.0001_bsz_26_trial_0_diffAugAllAug_cosine/models/last.pth\", None],\n",
    "               \"SupCon_baseline\": [\"./save/SupCon/animals10_diff_-1/SupCon_animals10_diff_-1_resnet18_lr_0.125_decay_0.0001_bsz_26_temp_0.1_trial_0_try3_cosine/models/last.pth\", \"\"],\n",
    "               \"SupCon_diff4000NoAug\": [\"./save/SupCon/animals10_diff_-1+4000/SupCon_animals10_diff_-1+4000_resnet18_lr_0.125_decay_0.0001_bsz_26_temp_0.1_trial_0_noAug_cosine/models/last.pth\", \"\"],\n",
    "               \"SupCon_diffCSameSAug\": [\"./save/SupCon/animals10_diff_-1+4000/SupCon_animals10_diff_-1+4000_resnet18_lr_0.125_decay_0.0001_bsz_26_temp_0.1_trial_0_colorAugSameShapeAug_cosine/models/last.pth\", \"\"],\n",
    "               \"SupCon_diffAllSameAug\": [\"./save/SupCon/animals10_diff_-1+4000/SupCon_animals10_diff_-1+4000_resnet18_lr_0.125_decay_0.0001_bsz_26_temp_0.1_trial_0_allSameAug_cosine/models/ckpt_epoch_150.pth\", \"\"],\n",
    "               \"SupConHybrid_diffNoAug\": [\"./save/SupCon/animals10_diff_-1+4000/SupConHybrid_animals10_diff_-1+4000_resnet18_lr_0.125_decay_0.0001_bsz_26_temp_0.1_trial_0_noAug_cosine/models/last.pth\", \"\"],\n",
    "               \"SupConHybrid_diffColorAug\": [\"./save/SupCon/animals10_diff_-1+4000/SupConHybrid_animals10_diff_-1+4000_resnet18_lr_0.125_decay_0.0001_bsz_26_temp_0.1_trial_0_colorAug_cosine/models/last.pth\", \"\"],\n",
    "               \"SupConFactor_factor5cAugSameSAug\": [\"./save/SupCon/animals10_diff_-1+4000/SupCon_5.0_animals10_diff_-1+4000_resnet18_lr_0.125_decay_0.0001_bsz_26_temp_0.1_trial_0_factor5cAugSameSAug_cosine/models/last.pth\", \"\"],\n",
    "               \"CE_city\": [\"./save/SupCE/city_classification_original/SupCE_city_classification_original_resnet18_lr_0.125_decay_0.0001_bsz_26_trial_0_cityBaseline_cosine/models/last.pth\", None],\n",
    "               \"CE_cityDiff\": [\"./save/SupCE/city_classification_diff/SupCE_city_classification_diff_resnet18_lr_0.125_decay_0.0001_bsz_26_trial_0_cityDiff_cosine/models/last.pth\", None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = 0\n",
    "\n",
    "model_short_name = \"CE_baseline\"\n",
    "root_model = models_dict[model_short_name][0]\n",
    "\n",
    "root_dataset = \"./datasets/adaIN/shape_texture_conflict_animals10_many/\"\n",
    "\n",
    "path_folder, epoch = ut_val.get_paths_from_model_checkpoint(root_model)\n",
    "\n",
    "params = open_csv_file(os.path.join(path_folder, \"params.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Dataloader and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=params['mean'], std=params['std'])\n",
    "val_transform = transforms.Compose([transforms.Resize(params['size']), transforms.CenterCrop(params['size']), transforms.ToTensor(), normalize])\n",
    "\n",
    "conflict_dataset = ut_val.shapeTextureConflictDataset(root_dataset, val_transform)\n",
    "classes = conflict_dataset.classes\n",
    "\n",
    "conflict_dataloader = torch.utils.data.DataLoader(conflict_dataset, batch_size=params['batch_size'],\n",
    "                                                  shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "model = ut_val.set_model(root_model, params, cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 897/897 [04:41<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "_, embedding_size = model_dict[params['model']]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "embedding = np.array([])\n",
    "shape_labels = np.array([], dtype=int)\n",
    "texture_labels = np.array([], dtype=int)\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(conflict_dataloader):\n",
    "        images = images.cuda(device=cuda_device, non_blocking=True)\n",
    "\n",
    "        features = model.encoder(images)\n",
    "\n",
    "        embedding = np.append(embedding, features.cpu().numpy())\n",
    "        shape_labels = np.append(shape_labels, labels[0].numpy())\n",
    "        texture_labels = np.append(texture_labels, labels[1].numpy())\n",
    "\n",
    "embedding = embedding.reshape(-1, embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_texture_name_list = [path.replace(\".jpg\", '').split('/')[-3:] for path in conflict_dataset.paths]\n",
    "shapeName_textureName_list = [(s+'/'+n.split('_stylized_')[0], t+'/'+n.split('_stylized_')[1]) for s,t,n in shape_texture_name_list]\n",
    "\n",
    "shape_pairs = []\n",
    "shape_array = np.array([sN for sN,_ in shapeName_textureName_list])\n",
    "for sN in set(shape_array):\n",
    "    shape_indices = np.where(shape_array == sN)[0]\n",
    "    shape_pairs.append(shape_indices)\n",
    "\n",
    "texture_pairs = []\n",
    "texture_array = np.array([tN for _,tN in shapeName_textureName_list])\n",
    "for tN in set(texture_array):\n",
    "    texture_indices = np.where(texture_array == tN)[0]\n",
    "    texture_pairs.append(texture_indices)\n",
    "\n",
    "shape_pair_A = np.concatenate([np.tile(shape_pairs[i], reps=len(shape_pairs[i])-1) for i in range(len(shape_pairs))])\n",
    "shape_pair_B = np.concatenate([np.concatenate([np.roll(shape_pairs[i], shift=j) for j in range(1,len(shape_pairs[i]))]) for i in range(len(shape_pairs))])\n",
    "\n",
    "texture_pair_A = np.concatenate([np.tile(texture_pairs[i], reps=len(texture_pairs[i])-1) for i in range(len(texture_pairs))])\n",
    "texture_pair_B = np.concatenate([np.concatenate([np.roll(texture_pairs[i], shift=j) for j in range(1,len(texture_pairs[i]))]) for i in range(len(texture_pairs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation_score(embedding_A, embedding_B):\n",
    "    A = torch.tensor(embedding_A)\n",
    "    B = torch.tensor(embedding_B)\n",
    "\n",
    "    A_dm = A - A.mean(dim=0)\n",
    "    B_dm = B - B.mean(dim=0)\n",
    "\n",
    "    correlation = (A_dm.T * B_dm.T).sum(dim=1) / ((A_dm.T * A_dm.T).sum(dim=1) * (B_dm.T * B_dm.T).sum(dim=1)).sqrt()\n",
    "    correlation = torch.nan_to_num(correlation, nan=0.0)\n",
    "\n",
    "    return correlation.mean().item()\n",
    "\n",
    "def estimate_dims(correlation_scores, embedding_size):\n",
    "    scores = np.array(np.concatenate((correlation_scores, [1.0])))\n",
    "\n",
    "    m = np.max(scores)\n",
    "    e = np.exp(scores-m)\n",
    "    softmaxed = e / np.sum(e)\n",
    "\n",
    "    dim = embedding_size\n",
    "    dims = [int(s*dim) for s in softmaxed]\n",
    "    dims[-1] = dim - sum(dims[:-1])\n",
    "\n",
    "    return dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[90, 182, 240]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_score_shape = compute_correlation_score(embedding[shape_pair_A], embedding[shape_pair_B])\n",
    "corr_score_texture = compute_correlation_score(embedding[texture_pair_A], embedding[texture_pair_B])\n",
    "\n",
    "dims = estimate_dims([corr_score_shape, corr_score_texture], embedding_size)\n",
    "dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch: last\n",
    "- CE_baseline: [93, 179, 240]\n",
    "- CE_origAllAug: [99, 169, 244]\n",
    "- CE_diff4000: [89, 184, 239]\n",
    "- CE_diff8000: [90, 182, 240]\n",
    "- CE_diffAug: [90, 184, 238]\n",
    "- CE_diffAugAllAug: [96, 173, 243]\n",
    "- SupCon_baseline: [102, 159, 251]\n",
    "- SupCon_diff4000NoAug: [90, 182, 240]\n",
    "- SupCon_diffCSameSAug: [104, 158, 250]\n",
    "- SupConHybrid_diffColorAug: [105, 161, 246]\n",
    "- SupConFactor_factor5cAugSameSAug: [104, 157, 251]\n",
    "\n",
    "epoch: 150\n",
    "- SupConFactor_factor5cAugSameSAug: [100, 162, 250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## Compute Embeddings for ShuffledPatches Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch.nn.functional as nnf\n",
    "\n",
    "class ShufflePatches:\n",
    "    # inspired from https://stackoverflow.com/questions/66962837/shuffle-patches-in-image-batch\n",
    "    def __init__(self, patch_size):\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # unfold the tensor image\n",
    "        u = nnf.unfold(x, kernel_size=self.patch_size , stride=self.patch_size , padding=0)\n",
    "        # shuffle the patches in unfolded form\n",
    "        pu = u[:,torch.randperm(u.shape[-1])]\n",
    "        # fold the tensor back in its original form\n",
    "        f = nnf.fold(pu, x.shape[-2:], kernel_size=self.patch_size, stride=self.patch_size, padding=0)\n",
    "\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202/202 [01:03<00:00,  3.17it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_orig = \"animals10_diff_-1\"\n",
    "dataset_diff = \"animals10_diff_4000\"\n",
    "# dataset_orig = \"city_classification_original\"\n",
    "# dataset_diff = \"city_classification_diff\"\n",
    "\n",
    "_, path_embeddings_orig, path_embeddings_diff, _ = ut_val.get_paths_from_model_checkpoint(root_model, dataset_1=dataset_orig, dataset_2=dataset_diff)\n",
    "\n",
    "_, root_dataset_orig = ut_val.get_root_dataset(dataset=dataset_orig)\n",
    "\n",
    "normalize = transforms.Normalize(mean=params['mean'], std=params['std'])\n",
    "shufflePatchers_transform = transforms.Compose([transforms.ToTensor(), ShufflePatches(patch_size=30), normalize])\n",
    "\n",
    "shufflePatches_dataset = datasets.ImageFolder(root=root_dataset_orig,transform=shufflePatchers_transform)\n",
    "\n",
    "shufflePatches_dataloader = torch.utils.data.DataLoader(shufflePatches_dataset, batch_size=params['batch_size'],\n",
    "                                                        shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "model = ut_val.set_model(root_model, params, cuda_device)\n",
    "\n",
    "\n",
    "# compute shuffled patches embedding\n",
    "_, embedding_size = model_dict[params['model']]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "embedding_test_patch = np.array([])\n",
    "labels_test_patch = np.array([], dtype=int)\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(shufflePatches_dataloader):\n",
    "        images = images.cuda(device=cuda_device, non_blocking=True)\n",
    "\n",
    "        features = model.encoder(images)\n",
    "\n",
    "        embedding_test_patch = np.append(embedding_test_patch, features.cpu().numpy())\n",
    "        labels_test_patch = np.append(labels_test_patch, labels.numpy())\n",
    "\n",
    "embedding_test_patch = embedding_test_patch.reshape(-1, embedding_size)\n",
    "\n",
    "\n",
    "# compute original and diffused embeddings\n",
    "with open(os.path.join(path_embeddings_orig, \"embedding_test\"), 'rb') as f:\n",
    "    entry = pickle.load(f, encoding='latin1')\n",
    "    embedding_test_orig = entry['data']\n",
    "    class_labels_test = entry['labels']\n",
    "\n",
    "with open(os.path.join(path_embeddings_diff, \"embedding_test\"), 'rb') as f:\n",
    "    entry = pickle.load(f, encoding='latin1')\n",
    "    embedding_test_diff = entry['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202/202 [01:00<00:00,  3.32it/s]\n"
     ]
    }
   ],
   "source": [
    "patchesColor_transform = transforms.Compose([transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
    "                                             transforms.ToTensor(), ShufflePatches(patch_size=30), normalize])\n",
    "\n",
    "patchesColor_dataset = datasets.ImageFolder(root=root_dataset_orig,transform=patchesColor_transform)\n",
    "\n",
    "patchesColor_dataloader = torch.utils.data.DataLoader(patchesColor_dataset, batch_size=params['batch_size'],\n",
    "                                                        shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "embedding_test_patchesColor = np.array([])\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(patchesColor_dataloader):\n",
    "        images = images.cuda(device=cuda_device, non_blocking=True)\n",
    "\n",
    "        features = model.encoder(images)\n",
    "\n",
    "        embedding_test_patchesColor = np.append(embedding_test_patchesColor, features.cpu().numpy())\n",
    "\n",
    "embedding_test_patchesColor = embedding_test_patchesColor.reshape(-1, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202/202 [01:01<00:00,  3.31it/s]\n"
     ]
    }
   ],
   "source": [
    "_, root_dataset_diff = ut_val.get_root_dataset(dataset=dataset_diff)\n",
    "\n",
    "color_transform = transforms.Compose([transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
    "                                      transforms.ToTensor(), normalize])\n",
    "\n",
    "diff_dataset = datasets.ImageFolder(root=root_dataset_diff,transform=color_transform)\n",
    "\n",
    "diff_dataloader = torch.utils.data.DataLoader(diff_dataset, batch_size=params['batch_size'],\n",
    "                                                        shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "embedding_test_diffColorJitter = np.array([])\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(diff_dataloader):\n",
    "        images = images.cuda(device=cuda_device, non_blocking=True)\n",
    "\n",
    "        features = model.encoder(images)\n",
    "\n",
    "        embedding_test_diffColorJitter = np.append(embedding_test_diffColorJitter, features.cpu().numpy())\n",
    "\n",
    "embedding_test_diffColorJitter = embedding_test_diffColorJitter.reshape(-1, embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate Dimension for ShuffledPatches Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation_score(embedding_A, embedding_B):\n",
    "    A = torch.tensor(embedding_A)\n",
    "    B = torch.tensor(embedding_B)\n",
    "\n",
    "    A_dm = A - A.mean(dim=0)\n",
    "    B_dm = B - B.mean(dim=0)\n",
    "\n",
    "    correlation = (A_dm * B_dm).sum(dim=0) / ((A_dm * A_dm).sum(dim=0) * (B_dm * B_dm).sum(dim=0)).sqrt()\n",
    "    correlation = torch.nan_to_num(correlation, nan=0.0)\n",
    "\n",
    "    return correlation.mean().item()\n",
    "\n",
    "def estimate_dims(correlation_scores, embedding_size):\n",
    "    scores = np.array(np.concatenate((correlation_scores, [1.0])))\n",
    "\n",
    "    m = np.max(scores)\n",
    "    e = np.exp(scores-m)\n",
    "    softmaxed = e / np.sum(e)\n",
    "\n",
    "    dim = embedding_size\n",
    "    dims = [int(s*dim) for s in softmaxed]\n",
    "    dims[-1] = dim - sum(dims[:-1])\n",
    "\n",
    "    return dims\n",
    "\n",
    "dim_estimate_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: 135, texture:  91, color: 114, other: 172\n"
     ]
    }
   ],
   "source": [
    "corr_score_shape = compute_correlation_score(embedding_test_orig, embedding_test_diffColorJitter)\n",
    "corr_score_texture = compute_correlation_score(embedding_test_orig, embedding_test_patchesColor)\n",
    "corr_score_color = compute_correlation_score(embedding_test_diff, embedding_test_patch)\n",
    "\n",
    "dims = estimate_dims([corr_score_shape, corr_score_texture, corr_score_color], embedding_size)\n",
    "\n",
    "print(f\"shape: {dims[0]:>3}, texture: {dims[1]:>3}, color: {dims[2]:>3}, other: {dims[3]:>3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: 163, texture: 141, other: 208\n"
     ]
    }
   ],
   "source": [
    "corr_score_shape = compute_correlation_score(embedding_test_orig, embedding_test_diffColorJitter)\n",
    "corr_score_texture = compute_correlation_score(embedding_test_orig, embedding_test_patch)\n",
    "\n",
    "dims = estimate_dims([corr_score_shape, corr_score_texture], embedding_size)\n",
    "\n",
    "print(f\"shape: {dims[0]:>3}, texture: {dims[1]:>3}, other: {dims[2]:>3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: 131, texture: 113, color: 100, other: 168\n"
     ]
    }
   ],
   "source": [
    "corr_score_shape = compute_correlation_score(embedding_test_orig, embedding_test_diff)\n",
    "corr_score_texture = compute_correlation_score(embedding_test_orig, embedding_test_patch)\n",
    "corr_score_color = compute_correlation_score(embedding_test_diff, embedding_test_patch)\n",
    "\n",
    "dims = estimate_dims([corr_score_shape, corr_score_texture, corr_score_color], embedding_size)\n",
    "\n",
    "print(f\"shape: {dims[0]:>3}, texture: {dims[1]:>3}, color: {dims[2]:>3}, other: {dims[3]:>3}\")\n",
    "\n",
    "dim_estimate_dict[model_short_name] = dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape dims</th>\n",
       "      <th>texture dims</th>\n",
       "      <th>color dims</th>\n",
       "      <th>other dims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CE_city</th>\n",
       "      <td>122</td>\n",
       "      <td>115</td>\n",
       "      <td>101</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CE_cityDiff</th>\n",
       "      <td>131</td>\n",
       "      <td>113</td>\n",
       "      <td>100</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             shape dims  texture dims  color dims  other dims\n",
       "CE_city             122           115         101         174\n",
       "CE_cityDiff         131           113         100         168"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_est_dims = pd.DataFrame.from_dict(dim_estimate_dict, orient=\"index\", columns=[\"shape dims\", \"texture dims\", \"color dims\", \"other dims\"])\n",
    "df_est_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|             |   shape dims |   texture dims |   color dims |   other dims |\n",
      "|:------------|-------------:|---------------:|-------------:|-------------:|\n",
      "| CE_cityDiff |          131 |            113 |          100 |          168 |\n",
      "| CE_city     |          122 |            115 |          101 |          174 |\n"
     ]
    }
   ],
   "source": [
    "print(df_est_dims.sort_values(\"shape dims\", ascending=False).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                                  |   shape dims |   texture dims |   color dims |   other dims |\n",
    "|:---------------------------------|-------------:|---------------:|-------------:|-------------:|\n",
    "| SupConFactor_factor5cAugSameSAug |          165 |             87 |           87 |          173 |\n",
    "| SupConHybrid_diffColorAug        |          163 |             88 |           88 |          173 |\n",
    "| SupCon_diffCSameSAug             |          157 |             93 |           93 |          169 |\n",
    "| CE_diffAugAllAug                 |          156 |             94 |           95 |          167 |\n",
    "| SupCon_diff4000NoAug             |          153 |             99 |           98 |          162 |\n",
    "| CE_diffAug                       |          149 |            102 |          103 |          158 |\n",
    "| CE_diff4000                      |          135 |            110 |           99 |          168 |\n",
    "| CE_origAllAug                    |          133 |            103 |          103 |          173 |\n",
    "| SupConHybrid_diffNoAug           |          132 |            120 |          120 |          140 |\n",
    "| CE_diff8000                      |          131 |            112 |           98 |          171 |\n",
    "| CE_baseline                      |          130 |            110 |          109 |          163 |\n",
    "| SupCon_baseline                  |          129 |            106 |          100 |          177 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|             |   shape dims |   texture dims |   color dims |   other dims |\n",
    "|:------------|-------------:|---------------:|-------------:|-------------:|\n",
    "| CE_cityDiff |          131 |            113 |          100 |          168 |\n",
    "| CE_city     |          122 |            115 |          101 |          174 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
