{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style transfer with AdaIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "import util.util_validation as ut_val\n",
    "from networks.resnet_big import SupCEResNet, SupConResNet, LinearClassifier, model_dict\n",
    "from util.util_logging import open_csv_file\n",
    "\n",
    "seaborn.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = 0\n",
    "\n",
    "# dataset = \"city_classification_original\"\n",
    "# styles_path = \"./datasets/adaIN/paintings/selected_for_styletransfer/test/\"\n",
    "# output_path = \"./datasets/adaIN/stylized_city_classification/test/\"\n",
    "\n",
    "dataset = \"animals10_diff_-1\"\n",
    "styles_path = \"./datasets/adaIN/textures_animals10_many/\"\n",
    "output_path = \"./datasets/adaIN/shape_texture_conflict_animals10_many/\"\n",
    "\n",
    "# dataset = \"animals10_diff_-1\"\n",
    "# styles_path = \"./datasets/adaIN/paintings/selected_for_styletransfer/train/\"\n",
    "# output_path = \"./datasets/adaIN/stylized_animals10/train/\"\n",
    "\n",
    "\n",
    "tmp_shape = \"./datasets/adaIN/tmp/shape/\"\n",
    "tmp_style = \"./datasets/adaIN/tmp/style/\"\n",
    "\n",
    "adaIn_venv_interpreter = \"./../adain_venv/bin/python\"\n",
    "addIn_path = \"./../pytorch-AdaIN/\"\n",
    "\n",
    "adaIn_execution_file = os.path.join(addIn_path, \"test.py\")\n",
    "adaIn_execution_file_many = os.path.join(addIn_path, \"test_many_individual.py\")\n",
    "adaIn_vgg = os.path.join(addIn_path, \"models/vgg_normalised.pth\")\n",
    "adaIn_decoder = os.path.join(addIn_path, \"models/decoder.pth\")\n",
    "\n",
    "classes = ut_val.get_classes(dataset)\n",
    "root_train, root_test = ut_val.get_root_dataset(dataset)\n",
    "\n",
    "image_loader = datasets.folder.default_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute miss classified Shape Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = 0\n",
    "\n",
    "models_dict = {\"CE_baseline\": [\"./save/SupCE/animals10/SupCE_animals10_resnet18_lr_0.125_decay_0.0001_bsz_26_trial_0_baseline_cosine/models/last.pth\", None],\n",
    "               \"CE_diffAug\": [\"./save/SupCE/animals10_diff_-1+4000/SupCE_animals10_diff_-1+4000_resnet18_lr_0.125_decay_0.0001_bsz_26_trial_0_diffAug_cosine/models/last.pth\", None],\n",
    "               \"CE_diffAugAllAug\": [\"./save/SupCE/animals10_diff_-1+4000/SupCE_animals10_diff_-1+4000_resnet18_lr_0.125_decay_0.0001_bsz_26_trial_0_diffAugAllAug_cosine/models/last.pth\", None],\n",
    "               \"SupCon_baseline\": [\"./save/SupCon/animals10_diff_-1/SupCon_animals10_diff_-1_resnet18_lr_0.125_decay_0.0001_bsz_26_temp_0.1_trial_0_try3_cosine/models/last.pth\", \"\"],\n",
    "               \"SupCon_diffCSameSAug\": [\"./save/SupCon/animals10_diff_-1+4000/SupCon_animals10_diff_-1+4000_resnet18_lr_0.125_decay_0.0001_bsz_26_temp_0.1_trial_0_colorAugSameShapeAug_cosine/models/last.pth\", \"\"]}\n",
    "\n",
    "dataset_stConflict = \"./datasets/adaIN/shape_texture_conflict_animals10/\"\n",
    "\n",
    "exclude_original_dict = ut_val.compute_exclude_dict(models_dict, dataset_stConflict, cuda_device)\n",
    "\n",
    "csv_file = \"./datasets/adaIN/experiments/exclude_animals10.csv\"\n",
    "with open(csv_file, 'w') as f:\n",
    "    w = csv.DictWriter(f, exclude_original_dict)\n",
    "    w.writeheader()\n",
    "    w.writerow(exclude_original_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create texture shape conflict Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files_dataFrame(root, classes):\n",
    "    \"\"\"\n",
    "    Creates a pandas DataFrame for a image dataset of the form ./root/class/img.png\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root: str\n",
    "        The path to the dataset\n",
    "    classes: iterable\n",
    "        Containing the class names (folder names in the dataset)\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    : pandas.DataFrame\n",
    "    DataFrame with columns image for the file names of the images\n",
    "    and label for the integer class label.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i, c in enumerate(classes):\n",
    "        # get all image path for a class\n",
    "        img_paths = glob.glob(os.path.join(root, f\"{c}/*\"))\n",
    "\n",
    "        for img_path in img_paths:\n",
    "            img_file = img_path.replace(os.path.join(root, f\"{c}/\"), '')\n",
    "            images.append(img_file)\n",
    "            labels.append(i)\n",
    "\n",
    "    return pd.DataFrame.from_dict({'image': images, 'label': labels})\n",
    "\n",
    "def adaIN(path_shape, path_style, path_output, size=300,\n",
    "           adaIn_execution_file=adaIn_execution_file, adaIn_vgg=adaIn_vgg,\n",
    "           adaIn_decoder=adaIn_decoder, cuda_device=cuda_device):\n",
    "\n",
    "    os.makedirs(path_output, exist_ok=True)\n",
    "\n",
    "    adaIn_call = f\"CUDA_VISIBLE_DEVICES={cuda_device} \"\\\n",
    "               + f\"{adaIn_venv_interpreter} {adaIn_execution_file} \"\\\n",
    "               + (\"--content\" if os.path.isfile(path_shape) else \"--content_dir\") + f\" {path_shape} \"\\\n",
    "               + (\"--style\" if os.path.isfile(path_style) else \"--style_dir\") + f\" {path_style} \"\\\n",
    "               + f\"--content_size {size} --style_size {size} --crop \"\\\n",
    "               + f\"--output {path_output} --vgg {adaIn_vgg} --decoder {adaIn_decoder}\"\n",
    "    \n",
    "    os.system(adaIn_call)\n",
    "\n",
    "def adaIN_many_individual(path_shape, path_style, path_content_style_csv, path_output, size=300,\n",
    "           adaIn_execution_file_many=adaIn_execution_file_many, adaIn_vgg=adaIn_vgg,\n",
    "           adaIn_decoder=adaIn_decoder, cuda_device=cuda_device):\n",
    "\n",
    "    os.makedirs(path_output, exist_ok=True)\n",
    "\n",
    "    adaIn_call = f\"CUDA_VISIBLE_DEVICES={cuda_device} \"\\\n",
    "               + f\"{adaIn_venv_interpreter} {adaIn_execution_file_many} \"\\\n",
    "               + f\"--content_dir {path_shape} \"\\\n",
    "               + f\"--style_dir {path_style} \"\\\n",
    "               + f\"--content_style_csv {path_content_style_csv} \"\\\n",
    "               + f\"--content_size {size} --style_size {size} --crop \"\\\n",
    "               + f\"--output {path_output} --vgg {adaIn_vgg} --decoder {adaIn_decoder}\"\n",
    "    \n",
    "    os.system(adaIn_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stylize trainings data with paintings (in a way that for each class no painting is used more frequent than other ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "painting_paths = np.array(glob.glob(os.path.join(styles_path, \"*\")))\n",
    "painting_labels = np.arange(len(painting_paths))\n",
    "\n",
    "df_train = create_files_dataFrame(root_train, classes)\n",
    "\n",
    "df_train[\"painting_lable\"] = len(df_train)*[-1]\n",
    "df_train[\"painting\"] = len(df_train)*[\"\"]\n",
    "\n",
    "for l, c in enumerate(tqdm(classes)):\n",
    "    index_class = np.array(df_train[df_train.label == l].index)\n",
    "    index_class = index_class[np.random.permutation(len(index_class))]\n",
    "\n",
    "    if len(index_class) > len(painting_paths):\n",
    "        index_split = np.array_split(index_class, len(painting_paths))\n",
    "        painting_labels_select = painting_labels\n",
    "    else:\n",
    "        index_split = np.array_split(index_class, len(index_class))\n",
    "        painting_labels_select = painting_labels[np.random.permutation(len(painting_labels))][:len(index_class)]\n",
    "\n",
    "    for i, pl in enumerate(painting_labels_select):\n",
    "        df_train.loc[index_split[i],\"painting_lable\"] = pl\n",
    "        df_train.loc[index_split[i],\"painting\"] = os.path.split(painting_paths[pl])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [12:04<00:00, 72.46s/it]\n"
     ]
    }
   ],
   "source": [
    "path_content_style_csv = \"./datasets/adaIN/tmp/content_style_pair.csv\"\n",
    "\n",
    "for l, c in enumerate(tqdm(classes)):\n",
    "    path_content = os.path.join(root_train, c)\n",
    "    out_path = os.path.join(output_path, c)\n",
    "\n",
    "    df_train.query(f\"label == {l}\").sort_values(\"painting_lable\").reset_index(drop=True)[[\"image\", \"painting\"]].to_csv(\n",
    "        path_content_style_csv, index=False, header=False)\n",
    "    \n",
    "    adaIN_many_individual(path_shape=path_content, path_style=styles_path, path_content_style_csv=path_content_style_csv,\n",
    "                          path_output=out_path, cuda_device=cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- resize the output to $300\\times300$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4218/4218 [00:11<00:00, 377.50it/s]\n"
     ]
    }
   ],
   "source": [
    "resized_dataset = datasets.ImageFolder(root=output_path, transform=transforms.Resize(300))\n",
    "\n",
    "for i in tqdm(range(len(resized_dataset))):\n",
    "    img = resized_dataset[i][0]\n",
    "    img_path = resized_dataset.imgs[i][0]\n",
    "\n",
    "    img.save(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "- stylize the dataset with paintings select at paintings random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_kinds = np.array([style_kind.split('/')[-1] for style_kind in glob.glob(os.path.join(styles_path, \"*\"))])\n",
    "style_paths = glob.glob(os.path.join(styles_path, \"*\", \"*\"))\n",
    "\n",
    "df_test = create_files_dataFrame(root_test, classes)\n",
    "\n",
    "N = int(np.ceil(len(df_test) / len(style_paths)))\n",
    "style_index_shuffled = np.repeat(np.arange(len(style_paths)), repeats=N)[np.random.permutation(len(style_paths)*N)]\n",
    "\n",
    "df_test[\"style_index\"] = style_index_shuffled[:len(df_test)]\n",
    "df_test[\"style_image\"] = [style_paths[i].split('/')[-1] for i in style_index_shuffled[:len(df_test)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [04:56<00:00,  5.49s/it]\n",
      "100%|██████████| 54/54 [04:26<00:00,  4.94s/it]\n",
      "100%|██████████| 54/54 [04:25<00:00,  4.92s/it]\n",
      "100%|██████████| 54/54 [03:37<00:00,  4.03s/it]\n",
      "100%|██████████| 54/54 [03:10<00:00,  3.52s/it]\n",
      "100%|██████████| 54/54 [03:16<00:00,  3.64s/it]\n",
      "100%|██████████| 54/54 [01:46<00:00,  1.97s/it]\n",
      "100%|██████████| 54/54 [02:23<00:00,  2.66s/it]\n",
      "100%|██████████| 54/54 [03:18<00:00,  3.67s/it]\n",
      "100%|██████████| 54/54 [03:05<00:00,  3.44s/it]\n",
      "100%|██████████| 54/54 [01:45<00:00,  1.96s/it]\n"
     ]
    }
   ],
   "source": [
    "for l, c in enumerate(classes):\n",
    "    for s_path in tqdm(style_paths):\n",
    "        s_e = s_path.split('/')[-1]\n",
    "\n",
    "        df_shape_style = df_test.query(f\"label == {l} & style_image == '{s_e}'\")\n",
    "        images = df_shape_style[\"image\"].values\n",
    "        if len(images) > 0:\n",
    "            out_path = os.path.join(output_path, c)\n",
    "\n",
    "            image_loader(s_path).save(os.path.join(tmp_style, s_path.split('/')[-1]))\n",
    "            \n",
    "            for img in images:\n",
    "                img_path = os.path.join(root_test, c, img)\n",
    "                image_loader(img_path).save(os.path.join(tmp_shape, img))\n",
    "            \n",
    "            adaIN(path_shape=tmp_shape, path_style=tmp_style, path_output=out_path, cuda_device=cuda_device)\n",
    "\n",
    "            tmp_files = glob.glob(os.path.join(tmp_shape, \"*\"))\n",
    "            tmp_files.extend(glob.glob(os.path.join(tmp_style, \"*\")))\n",
    "\n",
    "            for f in tmp_files:\n",
    "                os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- exclude miss classified images\n",
    "- stylize each shape with a texture of all other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"./datasets/adaIN/experiments/exclude_animals10.csv\"\n",
    "\n",
    "loaded_exclude_dict = open_csv_file(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_kinds = np.sort(np.array([style_kind.split('/')[-1] for style_kind in glob.glob(os.path.join(styles_path, \"*\"))]))\n",
    "style_paths = glob.glob(os.path.join(styles_path, \"*\", \"*\"))\n",
    "num_styles_per_shape = len(style_kinds) + (-1 if len(set(classes).intersection(style_kinds)) > 0 else 0)\n",
    "\n",
    "df_test = create_files_dataFrame(root_test, classes)\n",
    "\n",
    "image_names = np.array([img.split('.')[0] for img in df_test[\"image\"].values])\n",
    "\n",
    "keep_indices = []\n",
    "for c in loaded_exclude_dict:\n",
    "    l = np.where(np.array(classes) == c)[0][0]\n",
    "    excl_class_indices = df_test.query(f\"label=={l}\")[[\"image\"]].map(lambda img: img.split('.')[0] not in loaded_exclude_dict[c]).query(\"image\").index\n",
    "    keep_indices.extend(excl_class_indices)\n",
    "df_keep_test = df_test.iloc[keep_indices].copy().reset_index(drop=True)\n",
    "\n",
    "min_class_count = df_keep_test.groupby(\"label\").count().min()[\"image\"]\n",
    "df_stylize = df_keep_test.groupby(\"label\").sample(min_class_count).copy().reset_index(drop=True)\n",
    "\n",
    "for i in range(num_styles_per_shape):\n",
    "    df_stylize[f\"style_label_{i}\"] = len(df_stylize)*[-1]\n",
    "\n",
    "for l,c in enumerate(classes):\n",
    "    for i in range(num_styles_per_shape):\n",
    "        class_index = df_stylize[df_stylize.label == l].index\n",
    "        df_stylize.loc[class_index, f\"style_label_{i}\"] = df_stylize[df_stylize.label == l][[f\"style_label_{i}\"]].map(lambda _: i + (1 if i >= l else 0))\n",
    "\n",
    "df_stylize_list = [\n",
    "    df_stylize[[\"image\", \"label\", f\"style_label_{i}\"]].rename(columns={f\"style_label_{i}\": \"style_label\"})\n",
    "    for i in range(num_styles_per_shape)\n",
    "]\n",
    "df_stylize = pd.concat(df_stylize_list).sort_values([\"label\", \"image\", \"style_label\"]).reset_index(drop=True)\n",
    "\n",
    "df_stylize[f\"style_image\"] = len(df_stylize)*[\"\"]\n",
    "for tl,s in enumerate(style_kinds):\n",
    "    for l,c in enumerate(classes):\n",
    "        if s != c:\n",
    "            style_index = df_stylize.query(f\"label=={l} & style_label=={tl}\").index\n",
    "            if len(style_index) == 0:\n",
    "                print(s, c)\n",
    "            style_index = style_index[np.random.permutation(len(style_index))]\n",
    "\n",
    "            style_examples = [s_e.split('/')[-1] for s_e in glob.glob(os.path.join(styles_path, s, \"*\"))]\n",
    "            index_examples = np.array_split(style_index, len(style_examples))\n",
    "            for j, s_e in enumerate(style_examples):\n",
    "                df_stylize.loc[index_examples[j],\"style_image\"] = s_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [15:53<00:00,  4.24s/it]\n",
      "100%|██████████| 225/225 [15:38<00:00,  4.17s/it]\n",
      "100%|██████████| 225/225 [14:53<00:00,  3.97s/it]\n",
      "100%|██████████| 225/225 [15:33<00:00,  4.15s/it]\n",
      "100%|██████████| 225/225 [15:39<00:00,  4.18s/it]\n",
      "100%|██████████| 225/225 [14:51<00:00,  3.96s/it]\n",
      "100%|██████████| 225/225 [15:42<00:00,  4.19s/it]\n",
      "100%|██████████| 225/225 [15:25<00:00,  4.11s/it]\n",
      "100%|██████████| 225/225 [16:04<00:00,  4.29s/it]\n",
      "100%|██████████| 225/225 [14:57<00:00,  3.99s/it]\n"
     ]
    }
   ],
   "source": [
    "for l, c in enumerate(classes):\n",
    "    for s_path in tqdm(style_paths):\n",
    "        s_e = s_path.split('/')[-1]\n",
    "        df_shape_style = df_stylize.query(f\"label == {l} & style_image == '{s_e}'\")\n",
    "        images = df_shape_style[\"image\"].values\n",
    "        if len(images) > 0:\n",
    "            style_label = df_shape_style[\"style_label\"].values[0]\n",
    "            out_path = os.path.join(output_path, c, style_kinds[style_label])\n",
    "\n",
    "            image_loader(s_path).save(os.path.join(tmp_style, s_path.split('/')[-1]))\n",
    "            \n",
    "            for img in images:\n",
    "                img_path = os.path.join(root_test, c, img)\n",
    "                image_loader(img_path).save(os.path.join(tmp_shape, img))\n",
    "            \n",
    "            adaIN(path_shape=tmp_shape, path_style=tmp_style, path_output=out_path, cuda_device=cuda_device)\n",
    "\n",
    "            tmp_files = glob.glob(os.path.join(tmp_shape, \"*\"))\n",
    "            tmp_files.extend(glob.glob(os.path.join(tmp_style, \"*\")))\n",
    "\n",
    "            for f in tmp_files:\n",
    "                os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- resize the output to $300\\times300$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23310/23310 [00:53<00:00, 436.26it/s]\n"
     ]
    }
   ],
   "source": [
    "conflict_dataset = ut_val.shapeTextureConflictDataset(root=output_path, transform=transforms.Resize(300))\n",
    "\n",
    "for i in tqdm(range(len(conflict_dataset))):\n",
    "    img = conflict_dataset[i][0]\n",
    "    img_path = conflict_dataset.paths[i]\n",
    "\n",
    "    img.save(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stylize each shape with one random selected texture of an other class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 79.93it/s]\n"
     ]
    }
   ],
   "source": [
    "style_kinds = np.array([style_kind.split('/')[-1] for style_kind in glob.glob(os.path.join(styles_path, \"*\"))])\n",
    "style_paths = glob.glob(os.path.join(styles_path, \"*\", \"*\"))\n",
    "\n",
    "df_test = create_files_dataFrame(root_test, classes)\n",
    "\n",
    "df_test[\"style_label\"] = len(df_test)*[-1]\n",
    "df_test[\"style_image\"] = len(df_test)*[\"\"]\n",
    "\n",
    "for l, c in enumerate(tqdm(classes)):\n",
    "    index_class = np.array(df_test[df_test.label == l].index)\n",
    "    index_class = index_class[np.random.permutation(len(index_class))]\n",
    "    index_split = np.array_split(index_class, len(style_kinds) + (-1 if c in style_kinds else 0))\n",
    "\n",
    "    style_labels = np.arange(len(style_kinds))\n",
    "    style_labels = style_labels[style_kinds != c]\n",
    "\n",
    "    for i, s in enumerate(style_kinds[style_kinds != c]):\n",
    "        df_test.loc[index_split[i],\"style_label\"] = style_labels[i]\n",
    "\n",
    "        style_examples = [s_e.split('/')[-1] for s_e in glob.glob(os.path.join(styles_path, s, \"*\")) ]\n",
    "\n",
    "        index_examples = np.array_split(index_split[i], len(style_examples))\n",
    "        for j, s_e in enumerate(style_examples):\n",
    "            df_test.loc[index_examples[j],\"style_image\"] = s_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:26<00:00,  4.33s/it]\n",
      "100%|██████████| 20/20 [01:13<00:00,  3.69s/it]\n",
      "100%|██████████| 20/20 [01:07<00:00,  3.36s/it]\n",
      "100%|██████████| 20/20 [01:10<00:00,  3.55s/it]\n",
      "100%|██████████| 20/20 [01:16<00:00,  3.82s/it]\n",
      "100%|██████████| 20/20 [01:08<00:00,  3.42s/it]\n",
      "100%|██████████| 20/20 [01:08<00:00,  3.40s/it]\n",
      "100%|██████████| 20/20 [01:08<00:00,  3.42s/it]\n",
      "100%|██████████| 20/20 [01:25<00:00,  4.25s/it]\n",
      "100%|██████████| 20/20 [01:08<00:00,  3.45s/it]\n"
     ]
    }
   ],
   "source": [
    "for l, c in enumerate(classes):\n",
    "    for s_path in tqdm(style_paths):\n",
    "        s_e = s_path.split('/')[-1]\n",
    "        df_shape_style = df_test.query(f\"label == {l} & style_image == '{s_e}'\")\n",
    "        images = df_shape_style[\"image\"].values\n",
    "        if len(images) > 0:\n",
    "            style_label = df_shape_style[\"style_label\"].values[0]\n",
    "            out_path = os.path.join(output_path, c, style_kinds[style_label])\n",
    "\n",
    "            image_loader(s_path).save(os.path.join(tmp_style, s_path.split('/')[-1]))\n",
    "            \n",
    "            for img in images:\n",
    "                img_path = os.path.join(root_test, c, img)\n",
    "                image_loader(img_path).save(os.path.join(tmp_shape, img))\n",
    "            \n",
    "            adaIN(path_shape=tmp_shape, path_style=tmp_style, path_output=out_path, cuda_device=cuda_device)\n",
    "\n",
    "            tmp_files = glob.glob(os.path.join(tmp_shape, \"*\"))\n",
    "            tmp_files.extend(glob.glob(os.path.join(tmp_style, \"*\")))\n",
    "\n",
    "            for f in tmp_files:\n",
    "                os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
